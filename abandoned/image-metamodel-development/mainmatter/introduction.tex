\section{Introduction}

In the last two decades, image detection systems have seen practical applications within various domains \citep{Tsai:2010cn,Girod:2011gw,Takacs:2008cg,Sivic:2003tj,Lowe:2004kp,Bay:2008ud,Jin:2016jd,Smeulders:2000tx}, but only recently has there been an apparent shift from heuristic-driven systems to machine-learning ones. Since the recent advancement of \glspl{cnn} from handwritten characters \citep{Lecun:1998hy} to object detection \citep{Krizhevsky:2012wl} using the ImageNet dataset in \citeyear{Krizhevsky:2012wl}, there is increasing interest to improve the \gls{cnn} \cite{Girshick:2014jx,Girshick:2015vr,Ren:2017ug,He:2017ud} for such purposes. As it is typical for texture-based classifiers in image detection (such as \glspl{cnn}) to require thousands of training samples \cite{Shivakumara:2011dn,Chen:2004ux}, providing \textit{quality} annotations is therefore critical: significant flaws due to poor annotation directly affects \gls{ai} models trained with such data.

Not all training data is initially perfect, and therefore a cognitive aid that tracks data lineage helps us understand the flow of data to \gls{ai} models: where is the data sourced from; how is it enriched, curated or updated; how does this affect the \gls{ai} model; and how do we maintain a record of such changes? Without provenance tracking, tracing defects in future models is challenging. This paper proposes a methodology used to develop a novel annotation metamodel that offers a conceptual vocabulary and organising principle to improve the quality and efficiency of labelling training data. 