\documentclass[a4paper,11pt]{article}

\input{simple_preamble.tex}
\title{Explainable Artificial Intelligence and User Acceptance}

\begin{document}
  
\maketitle

\section{Introduction}

What makes outcomes inferred from artificial intelligence (AI) trusting? How do we convincingly communicate results inferred from an AI model to the end-user? Moreover, what types of stakeholders exist, and how would these explanations differ between each? As our AI models improve, we must take a step back and \textit{explain} the outcomes we infer. We must, therefore, begin to embark in a new chapter of AI that makes our models accountable for. Explainable AI (XAI) is this new chapter where we question the outcomes of AI models to reason why the outcome is true, and thus AI for businesses and developers (or any non-AI expert) to accept of such outcomes becomes an integral challenge.

To make an outcome accepting, the explanation must be convincing. How does one make a convincing explanation, and what is the best language to use to explain to the right end user? Studies show explanations are imperative to Decision Support Systems (DSS), and so if an AI is to be used to support decision-making, it must have a thorough but clear explanation to solidify any claim. Therefore, the communication of these explanations are more important than the outcomes themselves.

To make the communication clear, we focus on what makes AI `intelligent' in the first place. This is approximately defined as a perception of \textit{human}-like intelligence: an AI is only as good as a human trains it to be. Thus, if it is a perception of human intelligence, then what are the attributes of human intelligence? Humans learn from theories that govern specific domains, theories that are gathered and expanded upon over centuries. Yet, these theories are generally based on inductive reasoning on our world: we produce theories through observation and, likewise, so does AI based on what it observes. What XAI aims to solve is the \textit{deductive} reasoning process: given an output, explain the theory.

% ``Instead of trying to produce a programme to simulate the adult mind, why not rather try to produce one which simulates the child's?'' -- Instead of modelling the adult mind, why not model the child mind. Intelligence. Turing 1950. Distinctive learning capacity of children... not only what adults can do, but learn as children can.
% 

\section{Case Studies}

\subsection{Executive Management}

% How do I explain in lay-mans terms?
% Visually?

\subsection{Software Engineers}

% How do I choose between different AI models?
% Which model has the best explanation?

\end{document}